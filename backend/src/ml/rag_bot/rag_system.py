"""
RAG System
"""

from typing import List, Dict, Any, Optional

# –ò–º–ø–æ—Ä—Ç—ã —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ/–∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
try:
    from .embeddings_service import EmbeddingsService
    from .vector_store import VectorStore
    from .document_loader import DocumentLoader, Document
    from .text_splitter import TextSplitter
    from .llm_service import LLMService, ChatMessage
except ImportError:
    # –ï—Å–ª–∏ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –∏–º–ø–æ—Ä—Ç—ã –Ω–µ —Ä–∞–±–æ—Ç–∞—é—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
    from ml.rag_bot.embeddings_service import EmbeddingsService
    from ml.rag_bot.vector_store import VectorStore
    from ml.rag_bot.document_loader import DocumentLoader, Document
    from ml.rag_bot.text_splitter import TextSplitter
    from ml.rag_bot.llm_service import LLMService, ChatMessage

class RAGSystem:
    def __init__(self, 
                embeddings_model: str = "cointegrated/rubert-tiny2",
                llm_provider: str = "local",
                llm_model: str = "microsoft/DialoGPT-medium",
                chunk_size: int = 1000,
                 overlap: int = 200):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã
        
        Args:
            embeddings_model: –ú–æ–¥–µ–ª—å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
            llm_provider: –ü—Ä–æ–≤–∞–π–¥–µ—Ä LLM ('openai', 'anthropic', 'local')
            llm_model: –ú–æ–¥–µ–ª—å LLM
            chunk_size: –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ –¥–ª—è —Ä–∞–∑–±–∏–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
            overlap: –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
        """
        print("üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RAG —Å–∏—Å—Ç–µ–º—ã...")

                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.embeddings_service = EmbeddingsService(embeddings_model)
        self.vector_store = VectorStore()
        self.document_loader = DocumentLoader()
        self.text_splitter = TextSplitter(chunk_size=chunk_size, overlap=overlap)
        self.llm_service = LLMService(provider=llm_provider, model=llm_model)
        
        print("‚úÖ RAG —Å–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ")

    def load_documents(self, file_paths: List[str]) -> Dict[str, Any]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        
        Args:
            file_paths: –°–ø–∏—Å–æ–∫ –ø—É—Ç–µ–π –∫ —Ñ–∞–π–ª–∞–º
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–≥—Ä—É–∑–∫–∏ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏
        """
        print(f"üìö –ó–∞–≥—Ä—É–∂–∞–µ–º {len(file_paths)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")


        total_documents = 0
        total_chunks = 0 
        errors = []

        for file_path in file_paths:
            try:
                # Load Document
                document = self.document_loader.load_file(file_path)
                total_documents += 1

                # Split into chunks
                chunks = self.text_splitter.split_text(document.content)
                total_chunks += len(chunks)

                # Create embeddings
                embeddings = self.embeddings_service.encode_batch(chunks)

                # Add to vector store
                metadata_list = []

                for i, chunk in enumerate(chunks):
                    metadata = {
                        **document.metadata,
                        "chunk_index": i,
                        "chunk_size": len(chunk),
                        "total_chunks": len(chunks),
                    }
                    metadata_list.append(metadata)

                # add vector db
                self.vector_store.add_documents(chunks, embeddings, metadata_list)

                print(f"‚úÖ {document.metadata['filename']}: {len(chunks)} —á–∞–Ω–∫–æ–≤")
            
            except Exception as e:
                error_msg = f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {file_path}: {e}"
                errors.append(error_msg)
                print(f"‚ùå {error_msg}")
        
        result = {
            "total_documents": total_documents,
            "total_chunks": total_chunks,
            "errors": errors,
            "success": len(errors) == 0
        }
        
        print(f"üìä –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {total_documents} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, {total_chunks} —á–∞–Ω–∫–æ–≤")
        return result
                
    def ask(self, question: str, top_k: int = 5) -> Dict[str, Any]:
        """
        –ó–∞–¥–∞—Ç—å –≤–æ–ø—Ä–æ—Å RAG —Å–∏—Å—Ç–µ–º–µ
        
        Args:
            question: –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞
            
        Returns:
            –û—Ç–≤–µ—Ç —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        print(f"üí¨ –ó–∞–¥–∞–µ–º –≤–æ–ø—Ä–æ—Å: {question}")

        try:
            # Search for vector db
            question_embedding = self.embeddings_service.encode(question)

            # Search for relevant documents
            search_results = self.vector_store.search(question_embedding, top_k=top_k)

            
            if not search_results:
                return {
                    "answer": "–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π.",
                    "sources": [],
                    "question": question,
                    "status": "no_results"
                }
            
            # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            context_parts = []
            sources = []

            for result in search_results:
                context_parts.append(result['document'])
                sources.append({
                    "id": result['id'],
                    "similarity": result['similarity'],
                    "metadata": result['metadata']
                })
            
            context = "\n\n".join(context_parts)

            # generate answer
            answer = self.llm_service.generate_rag_response(question, context, sources)

            result = {
                "answer": answer,
                "sources": sources,
                "question": question,
                "context_length": len(context),
                "sources_count": len(sources),
                "status": "success"
            }

            print(f"‚úÖ –û—Ç–≤–µ—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ {len(sources)} –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
            return result
            
        except Exception as e:
            error_msg = f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤–æ–ø—Ä–æ—Å–∞: {e}"
            print(f"‚ùå {error_msg}")
            
            return {
                "answer": "–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å.",
                "sources": [],
                "question": question,
                "error": error_msg,
                "status": "error"
            }

    
    def chat(self, message: str, history: List[Dict[str, str]] = None) -> Dict[str, Any]:
        """
        –ß–∞—Ç —Å —Å–∏—Å—Ç–µ–º–æ–π (—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏)
        
        Args:
            message: –ù–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
            history: –ò—Å—Ç–æ—Ä–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π [{"role": "user/assistant", "content": "..."}]
            
        Returns:
            –û—Ç–≤–µ—Ç —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """

        if history is None:
            history = []
        
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –æ—Ç–≤–µ—Ç–∏—Ç—å —á–µ—Ä–µ–∑ RAG
        rag_response = self.ask(message)


        # –ï—Å–ª–∏ RAG –¥–∞–ª —Ö–æ—Ä–æ—à–∏–π –æ—Ç–≤–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ–≥–æ
        if rag_response["status"] == "success" and rag_response["sources_count"] > 0:
            return {
                "answer": rag_response["answer"],
                "sources": rag_response["sources"],
                "type": "rag",
                "status": "success"
            }

        #else using just chat
        messages = []

        # Add history
        for msg in history[-5:]: #limit to last 5 messages
            messages.append(ChatMessage(role=msg["role"], content=msg["content"]))

        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        messages.append(ChatMessage(role="user", content=message))
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        answer = self.llm_service.generate_chat_response(messages)
        
        return {
            "answer": answer,
            "sources": [],
            "type": "chat",
            "status": "success"
        }
        
    def search(self, query: str, top_k: int = 10) -> List[Dict]:
        """
        –ü–æ–∏—Å–∫ –≤ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑–µ –±–µ–∑ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            top_k: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        """
        try:
            query_embedding = self.embeddings_service.encode(query)
            results = self.vector_store.search(query_embedding, top_k=top_k)
            
            return results
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞: {e}")
            return []
    def get_status(self) -> Dict[str, Any]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–∏—Å—Ç–µ–º—ã
        
        Returns:
            –°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã
        """
        return {
            "embeddings_service": {
                "model": self.embeddings_service.model_name,
                "status": "ready"
            },
            "vector_store": self.vector_store.get_collection_info(),
            "llm_service": self.llm_service.get_status(),
            "text_splitter": {
                "chunk_size": self.text_splitter.chunk_size,
                "overlap": self.text_splitter.overlap
            },
            "system_status": "ready"
        }
    def clear_database(self) -> bool:
        """–û—á–∏—Å—Ç–∏—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω—É—é –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö"""
        return self.vector_store.clear_collection()
